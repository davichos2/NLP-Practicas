{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Práctica 4. Similitud de palabras y oraciones\n",
    "Ultima modificación: 10/06/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar librerias\n",
    "import re \n",
    "import nltk\n",
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "import transformers\n",
    "import torch\n",
    "from nltk.probability import FreqDist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar los documentos.\n",
    "with open('libros/The Adventures of Sherlock Holmes.txt' ,'r', encoding = 'utf-8') as archivo:\n",
    "    libro1 = archivo.read()\n",
    "\n",
    "with open('libros/The Last Man.txt' ,'r', encoding = 'utf-8') as archivo:\n",
    "    libro2 = archivo.read()\n",
    "\n",
    "with open('libros/Frankenstein.txt' ,'r', encoding = 'utf-8') as archivo:\n",
    "    libro3 = archivo.read()\n",
    "\n",
    "with open('libros/The War of the Worlds.txt' ,'r', encoding = 'utf-8') as archivo:\n",
    "    libro4 = archivo.read()\n",
    "\n",
    "with open('libros/THE SECRET ADVERSARY.txt' ,'r', encoding = 'utf-8') as archivo:\n",
    "    libro5 = archivo.read()\n",
    "\n",
    "with open('libros/The Mysterious Affair at Styles.txt' ,'r', encoding = 'utf-8') as archivo:\n",
    "    libro6 = archivo.read()\n",
    "\n",
    "with open('libros/MOBY-DICK;.txt' ,'r', encoding = 'utf-8') as archivo:\n",
    "    libro7 = archivo.read()\n",
    "\n",
    "with open('libros/Alice’s Adventures in Wonderland.txt', 'r', encoding = 'utf-8') as archivo:\n",
    "    libro8 = archivo.read()\n",
    "\n",
    "with open('libros/DRACULA.txt', 'r', encoding= 'utf-8') as archivo:\n",
    "    libro9 = archivo.read()\n",
    "\n",
    "with open('libros/The Phantom of the Opera.txt', 'r', encoding= 'utf-8') as archivo:\n",
    "    libro10 =  archivo.read()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover indiceS\n",
    "libro1 = re.sub(r'Contents.*?(?=I.\\n)', '', libro1, flags=re.DOTALL)\n",
    "libro2 = re.sub(r'Contents.*?(?=I visited)', '', libro2, flags=re.DOTALL)\n",
    "libro3 = re.sub(r'CONTENTS.*?(?=England._\\n)', '', libro3, flags=re.DOTALL)\n",
    "libro4 = re.sub(r'But who shall dwell.*?(?=No one would)', '', libro4, flags=re.DOTALL)\n",
    "libro5 = re.sub(r'CONTENTS.*?AND AFTER', '', libro5, flags=re.DOTALL)\n",
    "libro6 = re.sub(r'Contents.*?(?=EXPLAINS\\n)', '', libro6, flags=re.DOTALL)\n",
    "libro7 = re.sub(r'CONTENTS.*?(?=Call me Ishmael)', '', libro7, flags=re.DOTALL)\n",
    "libro8 = re.sub(r'THE MILLENNIUM FULCRUM EDITION 3\\.0.*?(?=Alice was)', '', libro8, flags= re.DOTALL)\n",
    "libro9 = re.sub(r'Contents.*?(?=DRACULA\\n)', '', libro9, flags=re.DOTALL)\n",
    "libro10 = re.sub(r'Contents.*?(?=\\{plus a \"bonus chapter\" called \"THE PARIS OPERA HOUSE\"\\}\\n)', '', libro10, flags=re.DOTALL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Extacción del primer capitulo y creación del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['capitulo1libro1.txt', 'capitulo1libro2.txt', 'capitulo1libro3.txt', 'capitulo1libro4.txt', 'capitulo1libro5.txt', 'capitulo1libro6.txt', 'capitulo1libro7.txt', 'capitulo1libro8.txt', 'capitulo1libro9.txt', 'capitulo1libro10.txt']\n"
     ]
    }
   ],
   "source": [
    "#Extraer el primer capitulo de cada libro\n",
    "patron = re.compile(r\"I.\\n(.*?)II\\.\", re.DOTALL)\n",
    "resultado1 = patron.search(libro1)\n",
    "\n",
    "patron = re.compile(r\"I am the native of.*?II\", re.DOTALL)\n",
    "resultado2 = patron.search(libro2)\n",
    "\n",
    "patron = re.compile(r\"Chapter 1.*?Chapter 2\", re.DOTALL)\n",
    "resultado3 = patron.search(libro3)\n",
    "\n",
    "patron = re.compile(r\"No one would.*?II\", re.DOTALL)\n",
    "resultado4 = patron.search(libro4)\n",
    "\n",
    "patron = re.compile(r\"CHAPTER I.*?CHAPTER II\", re.DOTALL)\n",
    "resultado5 = patron.search(libro5)\n",
    "\n",
    "patron = re.compile(r\"CHAPTER I.*?CHAPTER II\", re.DOTALL)\n",
    "resultado6 = patron.search(libro6)\n",
    "\n",
    "patron = re.compile(r\"Call me Ishmael(.*?)CHAPTER 2\", re.DOTALL)\n",
    "resultado7 = patron.search(libro7)\n",
    "\n",
    "patron = re.compile(r\"Alice was.*?CHAPTER II\", re.DOTALL)\n",
    "resultado8 = patron.search(libro8)\n",
    "\n",
    "patron = re.compile(r\"CHAPTER I.*?CHAPTER II\", re.DOTALL)\n",
    "resultado9 = patron.search(libro9)\n",
    "\n",
    "patron = re.compile(r\"Chapter I.*?Chapter II\", re.DOTALL)\n",
    "resultado10 = patron.search(libro10)\n",
    "\n",
    "# Crear una carpeta para almacenar los capítulos\n",
    "os.makedirs('libros', exist_ok=True)\n",
    "\n",
    "# Función para guardar el primer capítulo en un archivo\n",
    "def guardar_capitulo(nombre_archivo, contenido):\n",
    "    if contenido:\n",
    "        with open(f'libros/{nombre_archivo}', 'w', encoding='utf-8') as f:\n",
    "            f.write(contenido)\n",
    "\n",
    "# Guardar los primeros capítulos en archivos separados\n",
    "guardar_capitulo('capitulo1libro1.txt', resultado1.group(1))\n",
    "guardar_capitulo('capitulo1libro2.txt', resultado2.group(0))\n",
    "guardar_capitulo('capitulo1libro3.txt', resultado3.group(0))\n",
    "guardar_capitulo('capitulo1libro4.txt', resultado4.group(0))\n",
    "guardar_capitulo('capitulo1libro5.txt', resultado5.group(0))\n",
    "guardar_capitulo('capitulo1libro6.txt', resultado6.group(0))\n",
    "guardar_capitulo('capitulo1libro7.txt', resultado7.group(0))\n",
    "guardar_capitulo('capitulo1libro8.txt', resultado8.group(0))\n",
    "guardar_capitulo('capitulo1libro9.txt', resultado9.group(0))\n",
    "guardar_capitulo('capitulo1libro10.txt', resultado10.group(0))\n",
    "\n",
    "# Crear un nuevo corpus con los primeros capítulos\n",
    "corpus_root = 'libros'\n",
    "capitulos = [f'capitulo1libro{i}.txt' for i in range(1, 11)]\n",
    "new_corpus = PlaintextCorpusReader(corpus_root, capitulos)\n",
    "\n",
    "# Verificar el contenido del corpus\n",
    "print(new_corpus.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Segmentación en oraciones y tokenizacion de acuerdo a su etiquetado gramatical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas_por_archivo = {}\n",
    "# Procesar cada archivo de capítulo por separado\n",
    "for archivo in capitulos:\n",
    "    # Leer el contenido del archivo\n",
    "    texto = new_corpus.raw(archivo)\n",
    "    \n",
    "    # Dividir el texto en oraciones\n",
    "    oraciones = nltk.sent_tokenize(texto)\n",
    "    \n",
    "    etiquetas_oraciones = []\n",
    "    \n",
    "    # Tokenizar y etiquetar gramaticalmente cada oración\n",
    "    for oracion in oraciones:\n",
    "        tokens = nltk.word_tokenize(oracion)\n",
    "        etiquetas = nltk.pos_tag(tokens)\n",
    "        etiquetas_oraciones.append(etiquetas)\n",
    "    \n",
    "    # Guardar las etiquetas gramaticales por archivo en el diccionario\n",
    "    etiquetas_por_archivo[archivo] = etiquetas_oraciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SIMILITUD SEMÁNTICA DE PALABRAS Y DOCUMENTOS\n",
    "\n",
    "\n",
    "##Verbos mas frecuentes y terminos similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: capitulo1libro1.txt\n",
      "Verbo más común encontrado: ('said', 21)\n",
      "\n",
      "Archivo: capitulo1libro2.txt\n",
      "Verbo más común encontrado: ('came', 5)\n",
      "\n",
      "Archivo: capitulo1libro3.txt\n",
      "Verbo más común encontrado: ('became', 7)\n",
      "\n",
      "Archivo: capitulo1libro4.txt\n",
      "Verbo más común encontrado: ('remember', 6)\n",
      "\n",
      "Archivo: capitulo1libro5.txt\n",
      "Verbo más común encontrado: ('know', 12)\n",
      "\n",
      "Archivo: capitulo1libro6.txt\n",
      "Verbo más común encontrado: ('said', 14)\n",
      "\n",
      "Archivo: capitulo1libro7.txt\n",
      "Verbo más común encontrado: ('go', 10)\n",
      "\n",
      "Archivo: capitulo1libro8.txt\n",
      "Verbo más común encontrado: ('see', 10)\n",
      "\n",
      "Archivo: capitulo1libro9.txt\n",
      "Verbo más común encontrado: ('seemed', 24)\n",
      "\n",
      "Archivo: capitulo1libro10.txt\n",
      "Verbo más común encontrado: ('said', 14)\n",
      "\n",
      "Terminos similares aplicando path_similarity\n",
      "\n",
      "Términos similares para el verbo más común del archivo capitulo1libro1.txt:\n",
      "['state', 'present', 'misstate', 'answer', 'precede']\n",
      "\n",
      "Términos similares para el verbo más común del archivo capitulo1libro2.txt:\n",
      "['come', 'emanate', 'address', 'travel', 'approach']\n",
      "\n",
      "Términos similares para el verbo más común del archivo capitulo1libro3.txt:\n",
      "['become', 'change_state', 'sober_up', 'sober_up', 'work']\n",
      "\n",
      "Términos similares para el verbo más común del archivo capitulo1libro4.txt:\n",
      "['remember', 'know', 'recognize', 'review', 'breathe']\n",
      "\n",
      "Términos similares para el verbo más común del archivo capitulo1libro5.txt:\n",
      "['know', 'keep_track', 'recognize', 'breathe', 'act']\n",
      "\n",
      "Términos similares para el verbo más común del archivo capitulo1libro6.txt:\n",
      "['state', 'present', 'misstate', 'answer', 'precede']\n",
      "\n",
      "Términos similares para el verbo más común del archivo capitulo1libro7.txt:\n",
      "['travel', 'go_around', 'carry', 'ease', 'whish']\n",
      "\n",
      "Términos similares para el verbo más común del archivo capitulo1libro8.txt:\n",
      "['see', 'perceive', 'glimpse', 'see', 'catch_sight']\n",
      "\n",
      "Términos similares para el verbo más común del archivo capitulo1libro9.txt:\n",
      "['look', 'make', 'cut', 'feel', 'pass_off']\n",
      "\n",
      "Términos similares para el verbo más común del archivo capitulo1libro10.txt:\n",
      "['state', 'present', 'misstate', 'answer', 'precede']\n",
      "\n",
      "\n",
      "\n",
      " Terminos similares utilizando wup_similarity\n",
      "\n",
      "['state', 'present', 'misstate', 'answer', 'precede']\n",
      "\n",
      "['come', 'emanate', 'address', 'approach', 'approach']\n",
      "\n",
      "['become', 'sober_up', 'sober_up', 'work', 'take_effect']\n",
      "\n",
      "['remember', 'know', 'recognize', 'review', 'breathe']\n",
      "\n",
      "['know', 'keep_track', 'recognize', 'breathe', 'act']\n",
      "\n",
      "['state', 'present', 'misstate', 'answer', 'precede']\n",
      "\n",
      "['travel', 'go_around', 'carry', 'ease', 'whish']\n",
      "\n",
      "['see', 'glimpse', 'see', 'catch_sight', 'behold']\n",
      "\n",
      "['look', 'make', 'cut', 'feel', 'pass_off']\n",
      "\n",
      "['state', 'present', 'misstate', 'answer', 'precede']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verbos_mas_comunes = []\n",
    "\n",
    "#Identificar el verbo más común en cada capítulo\n",
    "for archivo, etiquetas_oraciones in etiquetas_por_archivo.items():\n",
    "    print(f'Archivo: {archivo}')\n",
    "    verbos = []\n",
    "    for oracion in etiquetas_oraciones:\n",
    "        for palabra, etiqueta in oracion:\n",
    "            if etiqueta.startswith('VB') and palabra not in stopwords.words('english') and palabra not in ['’', \"'s\"]:\n",
    "                verbos.append(palabra)\n",
    "    fdist = FreqDist(verbos)\n",
    "    verbo_mas_comun = fdist.most_common(1)\n",
    "    \n",
    "    # Guardar el verbo más común en la lista\n",
    "    if verbo_mas_comun:\n",
    "        verbos_mas_comunes.append(verbo_mas_comun[0][0])\n",
    "    else:\n",
    "        verbos_mas_comunes.append(None)\n",
    "    \n",
    "    # Imprimir el verbo más común encontrado en el archivo\n",
    "    if verbo_mas_comun:\n",
    "        print(f'Verbo más común encontrado: {verbo_mas_comun[0]}')\n",
    "    else:\n",
    "        print('No se encontraron verbos en este archivo.')\n",
    "    \n",
    "    print()\n",
    "    \n",
    "\n",
    "# Función para obtener los términos más similares utilizando path_similarity\n",
    "def obtener_terminos_similares(verbo):\n",
    "    # Obtener los synsets del verbo en WordNet\n",
    "    synsets = wn.synsets(verbo, pos=wn.VERB)\n",
    "    \n",
    "    terminos_similares = []\n",
    "    \n",
    "    # Iterar sobre cada synset del verbo\n",
    "    for synset in synsets:\n",
    "        # Calcular la similitud de camino con otros synsets\n",
    "        similitudes = []\n",
    "        for otro_synset in wn.all_synsets(pos=wn.VERB):\n",
    "            similitud = synset.path_similarity(otro_synset)\n",
    "            if similitud is not None:\n",
    "                similitudes.append((otro_synset, similitud))\n",
    "        \n",
    "        # Ordenar por similitud y obtener los términos similares\n",
    "        similitudes.sort(key=lambda x: x[1], reverse=True)\n",
    "        terminos_similares.extend((syn.name().split('.')[0] for syn, _ in similitudes[:5]))\n",
    "    \n",
    "    return terminos_similares\n",
    "\n",
    "# Función para obtener los términos más similares utilizando path_similarity\n",
    "def obtener_terminos_similares2(verbo):\n",
    "    # Obtener los synsets del verbo en WordNet\n",
    "    synsets = wn.synsets(verbo, pos=wn.VERB)\n",
    "    \n",
    "    terminos_similares = []\n",
    "    \n",
    "    # Iterar sobre cada synset del sustantivo\n",
    "    for synset in synsets:\n",
    "        # Calcular la similitud de camino con otros synsets\n",
    "        similitudes = []\n",
    "        for otro_synset in wn.all_synsets(pos=wn.VERB):\n",
    "            similitud = synset.wup_similarity(otro_synset)\n",
    "            if similitud is not None:\n",
    "                similitudes.append((otro_synset, similitud))\n",
    "        \n",
    "        # Ordenar por similitud y obtener los términos similares\n",
    "        similitudes.sort(key=lambda x: x[1], reverse=True)\n",
    "        terminos_similares.extend((syn.name().split('.')[0] for syn, _ in similitudes[:5]))\n",
    "    \n",
    "    return terminos_similares\n",
    "\n",
    "\n",
    "# Crear una lista para almacenar los términos similares para cada verbo más común\n",
    "terminos_similares = []\n",
    "\n",
    "# Iterar sobre cada verbo más común utilizando path_similarity\n",
    "for verbo in verbos_mas_comunes:\n",
    "    terminos = obtener_terminos_similares(verbo)\n",
    "    terminos_similares.append(terminos)\n",
    "\n",
    "# Imprimir los términos similares para cada verbo más común\n",
    "print(f'Terminos similares aplicando path_similarity\\n')\n",
    "for i, terminos in enumerate(terminos_similares):\n",
    "    print(f'Términos similares para el verbo más común del archivo {capitulos[i]}:')\n",
    "    print(terminos[0:5])\n",
    "    print()\n",
    "\n",
    "terminos_similares2 = []\n",
    "# Iterar sobre cada verbo más común utilizando wup_similarity\n",
    "for verbo in verbos_mas_comunes:\n",
    "    terminos = obtener_terminos_similares2(verbo)\n",
    "    terminos_similares2.append(terminos)\n",
    "\n",
    "print('\\n\\n Terminos similares utilizando wup_similarity\\n')\n",
    "for i, terminos in enumerate(terminos_similares2):\n",
    "    #print(f'Términos similares para el verbo más común del archivo {capitulos[i]}:')\n",
    "    print(terminos[0:5])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sustantivos mas frecuentes y terminos similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: capitulo1libro1.txt\n",
      "Sustantivo más común encontrado: ('Holmes', 23)\n",
      "\n",
      "Archivo: capitulo1libro2.txt\n",
      "Sustantivo más común encontrado: ('father', 18)\n",
      "\n",
      "Archivo: capitulo1libro3.txt\n",
      "Sustantivo más común encontrado: ('father', 13)\n",
      "\n",
      "Archivo: capitulo1libro4.txt\n",
      "Sustantivo más común encontrado: ('planet', 14)\n",
      "\n",
      "Archivo: capitulo1libro5.txt\n",
      "Sustantivo más común encontrado: ('Tuppence', 29)\n",
      "\n",
      "Archivo: capitulo1libro6.txt\n",
      "Sustantivo más común encontrado: ('John', 25)\n",
      "\n",
      "Archivo: capitulo1libro7.txt\n",
      "Sustantivo más común encontrado: ('part', 7)\n",
      "\n",
      "Archivo: capitulo1libro8.txt\n",
      "Sustantivo más común encontrado: ('way', 11)\n",
      "\n",
      "Archivo: capitulo1libro9.txt\n",
      "Sustantivo más común encontrado: ('driver', 26)\n",
      "\n",
      "Archivo: capitulo1libro10.txt\n",
      "Sustantivo más común encontrado: ('ghost', 34)\n",
      "\n",
      "Terminos similares aplicando path_similarity\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro1.txt:\n",
      "['cognition', 'psychological_feature', 'abstraction', 'motivation', 'event']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro2.txt:\n",
      "['person', 'organism', 'causal_agent', 'physical_entity', 'living_thing']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro3.txt:\n",
      "['person', 'organism', 'causal_agent', 'physical_entity', 'living_thing']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro4.txt:\n",
      "['natural_object', 'whole', 'object', 'congener', 'living_thing']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro5.txt:\n",
      "['measure', 'abstraction', 'entity', 'psychological_feature', 'attribute']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro6.txt:\n",
      "['artifact', 'whole', 'article', 'facility', 'object']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro7.txt:\n",
      "['substance', 'relation', 'abstraction', 'possession', 'social_relation']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro8.txt:\n",
      "['attribute', 'abstraction', 'state', 'shape', 'time']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro9.txt:\n",
      "['causal_agent', 'physical_entity', 'person', 'entity', 'thing']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro10.txt:\n",
      "['cognition', 'psychological_feature', 'abstraction', 'motivation', 'event']\n",
      "\n",
      "\n",
      "\n",
      " Terminos similares utilizando wup_similarity\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro1.txt:\n",
      "['cognition', 'sleep_talking', 'psychological_feature', 'motivation', 'event']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro2.txt:\n",
      "['organism', 'benthos', 'dwarf', 'heterotroph', 'parent']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro3.txt:\n",
      "['organism', 'benthos', 'dwarf', 'heterotroph', 'parent']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro4.txt:\n",
      "['natural_object', 'whole', 'congener', 'living_thing', 'artifact']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro5.txt:\n",
      "['measure', 'bowling_score', 'football_score', 'point_after', 'baseball_score']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro6.txt:\n",
      "['artifact', 'article', 'facility', 'whole', 'congener']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro7.txt:\n",
      "['substance', 'relation', 'possession', 'social_relation', 'abstraction']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro8.txt:\n",
      "['attribute', 'state', 'shape', 'time', 'space']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro9.txt:\n",
      "['causal_agent', 'person', 'physical_entity', 'thing', 'object']\n",
      "\n",
      "Términos similares para el sustantivo más común del archivo capitulo1libro10.txt:\n",
      "['cognition', 'frame', 'play', 'original_sin', 'musical_performance']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_synsets_nouns = list(wn.all_synsets(pos=wn.NOUN))[:4000]  # Limitar a los primeros 4000\n",
    "\n",
    "# Función para obtener los términos más similares utilizando path_similarity\n",
    "def obtener_terminos_similares(sustantivo):\n",
    "    synsets = wn.synsets(sustantivo, pos=wn.NOUN)\n",
    "    terminos_similares = []\n",
    "\n",
    "    for synset in synsets:\n",
    "        similarities = []\n",
    "        for otro_synset in all_synsets_nouns:\n",
    "            similarity = synset.path_similarity(otro_synset)\n",
    "            if similarity is not None:\n",
    "                similarities.append((otro_synset, similarity))\n",
    "\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        terminos_similares.extend((syn.name().split('.')[0] for syn, _ in similarities[:5]))\n",
    "\n",
    "    return terminos_similares\n",
    "\n",
    "# Función para obtener los términos más similares utilizando wup_similarity\n",
    "def obtener_terminos_similares2(sustantivo):\n",
    "    synsets = wn.synsets(sustantivo, pos=wn.NOUN)\n",
    "    terminos_similares = []\n",
    "\n",
    "    for synset in synsets:\n",
    "        similarities = []\n",
    "        for otro_synset in all_synsets_nouns:\n",
    "            similarity = synset.wup_similarity(otro_synset)\n",
    "            if similarity is not None:\n",
    "                similarities.append((otro_synset, similarity))\n",
    "\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        terminos_similares.extend((syn.name().split('.')[0] for syn, _ in similarities[:5]))\n",
    "\n",
    "    return terminos_similares\n",
    "\n",
    "############################################################################################################################################################\n",
    "\n",
    "sustantivos_mas_comunes = []\n",
    "\n",
    "#Identificar el sustantivo más común en cada capítulo\n",
    "for archivo, etiquetas_oraciones in etiquetas_por_archivo.items():\n",
    "    print(f'Archivo: {archivo}')\n",
    "    sustantivos = []\n",
    "    for oracion in etiquetas_oraciones:\n",
    "        for palabra, etiqueta in oracion:\n",
    "            # Filtrar sustantivos comunes y eliminar palabras vacías y palabras no deseadas\n",
    "            if etiqueta.startswith('NN') and palabra not in stopwords.words('english') and palabra not in ['’', \"'s\", '“', '”', '*', 'Tommy', 'Alice']:\n",
    "                sustantivos.append(palabra)\n",
    "    # Calcular la frecuencia de los sustantivos\n",
    "    fdist = FreqDist(sustantivos)\n",
    "    sustantivo_mas_comun = fdist.most_common(1)\n",
    "    \n",
    "    # Guardar el sustantivo más común en la lista\n",
    "    if sustantivo_mas_comun:\n",
    "        sustantivos_mas_comunes.append(sustantivo_mas_comun[0][0])\n",
    "    else:\n",
    "        sustantivos_mas_comunes.append(None)\n",
    "    \n",
    "    # Imprimir el sustantivo más común encontrado en el archivo\n",
    "    if sustantivo_mas_comun:\n",
    "        print(f'Sustantivo más común encontrado: {sustantivo_mas_comun[0]}')\n",
    "    else:\n",
    "        print('No se encontraron sustantivos en este archivo.')\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Crear una lista para almacenar los términos similares para cada sustantivo más común\n",
    "terminos_similares = []\n",
    "\n",
    "# Iterar sobre cada sustantivo más común\n",
    "for sustantivo in sustantivos_mas_comunes:\n",
    "    if sustantivo is not None:  # Asegurarse de que el sustantivo no sea None\n",
    "        terminos = obtener_terminos_similares(sustantivo)\n",
    "        terminos_similares.append(terminos)\n",
    "    else:\n",
    "        terminos_similares.append([])\n",
    "\n",
    "# Imprimir los términos similares para cada sustantivo más común\n",
    "print(f'Terminos similares aplicando path_similarity')\n",
    "for i, terminos in enumerate(terminos_similares):\n",
    "    print(f'Términos similares para el sustantivo más común del archivo {capitulos[i]}:')\n",
    "    print(terminos[0:5])\n",
    "    print()\n",
    "\n",
    "\n",
    "print(f'\\n\\n Terminos similares utilizando wup_similarity\\n')\n",
    "terminos_similares2 = []\n",
    "# Iterar sobre cada sustantivo más común\n",
    "for sustantivo in sustantivos_mas_comunes:\n",
    "    if sustantivo is not None:  # Asegurarse de que el sustantivo no sea None\n",
    "        terminos = obtener_terminos_similares2(sustantivo)\n",
    "        terminos_similares2.append(terminos)\n",
    "    else:\n",
    "        terminos_similares2.append([])\n",
    "\n",
    "for i, terminos in enumerate(terminos_similares2):\n",
    "    print(f'Términos similares para el sustantivo más común del archivo {capitulos[i]}:')\n",
    "    print(terminos[0:5])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Similitud de frases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frases clave para el archivo capitulo1libro1.txt:\n",
      "They were admirable things for the observer—excellent for\n",
      "drawing the veil from men’s motives and actions.\n",
      "\n",
      "Frases clave para el archivo capitulo1libro2.txt:\n",
      "He was induced\n",
      "to look with extreme disapprobation, and at last with distaste, on my\n",
      "father’s imprudence and follies.\n",
      "\n",
      "Frases clave para el archivo capitulo1libro3.txt:\n",
      "Beaufort had taken effectual measures to conceal himself, and it was ten\n",
      "months before my father discovered his abode.\n",
      "\n",
      "Frases clave para el archivo capitulo1libro4.txt:\n",
      "The immediate pressure of necessity has brightened\n",
      "their intellects, enlarged their powers, and hardened their hearts.\n",
      "\n",
      "Frases clave para el archivo capitulo1libro5.txt:\n",
      "The place was full, and they wandered about looking for a table,\n",
      "catching odds and ends of conversation as they did so.\n",
      "\n",
      "\n",
      "\n",
      "Frases clave para el archivo capitulo1libro6.txt:\n",
      "Having no near relations or friends, I was trying to make\n",
      "up my mind what to do, when I ran across John Cavendish.\n",
      "\n",
      "Frases clave para el archivo capitulo1libro7.txt:\n",
      "For my part, I abominate all\n",
      "honorable respectable toils, trials, and tribulations of every kind\n",
      "whatsoever.\n",
      "\n",
      "Frases clave para el archivo capitulo1libro8.txt:\n",
      "There was nothing so _very_ remarkable in that; nor did Alice think it\n",
      "so _very_ much out of the way to hear the Rabbit say to itself, “Oh\n",
      "dear!\n",
      "\n",
      "Frases clave para el archivo capitulo1libro9.txt:\n",
      "I had for dinner, or\n",
      "rather supper, a chicken done up some way with red pepper, which was\n",
      "very good but thirsty.\n",
      "\n",
      "Frases clave para el archivo capitulo1libro10.txt:\n",
      "He had run up against\n",
      "the ghost on the little staircase, by the footlights, which leads to\n",
      "\"the cellars.\"  \n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro1.txt:\n",
      "0.2799339549339549\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro2.txt:\n",
      "0.2916876526251526\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro3.txt:\n",
      "0.3186244311244311\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro4.txt:\n",
      "0.3221958596958597\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro5.txt:\n",
      "0.3884424603174603\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro6.txt:\n",
      "0.37715773809523806\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro7.txt:\n",
      "0.33397435897435895\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro8.txt:\n",
      "0.38087641525141525\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro9.txt:\n",
      "1.0\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro10.txt:\n",
      "0.36116522366522363\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Función para extraer frases clave usando TextRank\n",
    "def top_words_spacy(text, max_phrase_length=10):\n",
    "    # Procesar el texto con spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Construir una lista de oraciones\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    # Calcular el puntaje de TextRank para cada oración\n",
    "    wordsSpacy = []\n",
    "    for sent in sentences:\n",
    "        phrase_doc = nlp(sent)\n",
    "        # Filtrar palabras vacías y signos de puntuación\n",
    "        phrase_tokens = [token.text for token in phrase_doc if not token.is_stop and not token.is_punct]\n",
    "        if len(phrase_tokens) <= max_phrase_length:\n",
    "            wordsSpacy.append((sent, len(phrase_tokens)))\n",
    "    \n",
    "    # Ordenar las frases clave por puntaje de TextRank\n",
    "    wordsSpacy = sorted(wordsSpacy, key=lambda x: x[1], reverse=True)\n",
    "    return wordsSpacy[0:1]\n",
    "\n",
    "\n",
    "guardar_frases_clave = []\n",
    "# Extraer frases clave para cada capítulo\n",
    "for archivo in capitulos:\n",
    "    texto = new_corpus.raw(archivo)\n",
    "    guardar_frases_clave.append(top_words_spacy(texto))\n",
    "\n",
    "# Imprimir las frases clave extraídas\n",
    "for i, frases_clave in enumerate(guardar_frases_clave):\n",
    "    print(f'Frases clave para el archivo {capitulos[i]}:')\n",
    "    for frase, _ in frases_clave:\n",
    "        print(frase)\n",
    "    print()\n",
    "\n",
    "#Libro elegido Dracula para comparar su frase clave con la de los demás libros\n",
    "frase_elegida = guardar_frases_clave[8]\n",
    "\n",
    "\n",
    "def convertirTags(tag):\n",
    "    # Convertir etiquetas POS de Penn Treebank a WordNet\n",
    "    tag_dict = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    try:\n",
    "        return tag_dict[tag[0]]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def convertirSynsets(frase):\n",
    "    # Convertir una frase en una lista de synsets\n",
    "    tokens = nltk.word_tokenize(frase)\n",
    "    post = nltk.pos_tag(tokens)\n",
    "    # Convertir etiquetas POS de Penn Treebank a WordNet\n",
    "    tags = [tag[1] for tag in post]\n",
    "    wntag = [convertirTags(tag) for tag in tags]\n",
    "    ans = list(zip(tokens, wntag))\n",
    "    # Obtener synsets\n",
    "    sets = [wn.synsets(x,y) for x,y in ans]\n",
    "    final = [val[0] for val in sets if len(val) > 0]\n",
    "    # return synsets\n",
    "    return final\n",
    "\n",
    "\n",
    "def compararFrases(frase1, frase2):\n",
    "    # Calcular la similitud de las frases clave\n",
    "    s =[]\n",
    "    for i1 in frase1:\n",
    "        r = []\n",
    "        # Comparar con cada synset de la otra frase\n",
    "        scores = [x for x in [i1.path_similarity(i2) for i2 in frase2] if x is not None]\n",
    "        if scores:\n",
    "            s.append(max(scores))\n",
    "    return sum(s)/len(s)\n",
    "\n",
    "synset1 = convertirSynsets(str(frase_elegida[0]))\n",
    "synset2 = convertirSynsets(str(guardar_frases_clave[0]))\n",
    "\n",
    "for i in range(0, 10):\n",
    "    synset2 = convertirSynsets(str(guardar_frases_clave[i]))\n",
    "    print(f\"Similitud entre las frases clave de Dracula y {capitulos[i]}:\")\n",
    "    print(compararFrases(synset1, synset2))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SIMILITUD SINTÁCTICA DE PALABRAS Y DOCUMENTOS\n",
    "\n",
    "##Similitud de palabras con “embedding”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['said', 'came', 'became', 'remember', 'know', 'said', 'go', 'see', 'seemed', 'said']\n",
      "Palabras similares para cada verbo más común:\n",
      "\n",
      "Palabras similares para el verbo más común del archivo capitulo1libro1.txt:\n",
      "told - Similitud: 0.8487459435498669\n",
      "says - Similitud: 0.8161521914923707\n",
      "saying - Similitud: 0.7887590350807986\n",
      "added - Similitud: 0.7826225953331531\n",
      "asked - Similitud: 0.7763409947242162\n",
      "\n",
      "Palabras similares para el verbo más común del archivo capitulo1libro2.txt:\n",
      "took - Similitud: 0.8730760801920413\n",
      "went - Similitud: 0.8710107969032627\n",
      "last - Similitud: 0.8637165318123398\n",
      "followed - Similitud: 0.8475635685297999\n",
      "again - Similitud: 0.846613954267577\n",
      "\n",
      "Palabras similares para el verbo más común del archivo capitulo1libro3.txt:\n",
      "becoming - Similitud: 0.8460562553656712\n",
      "become - Similitud: 0.8415305786329104\n",
      "was - Similitud: 0.8205906975345288\n",
      "as - Similitud: 0.7907937057505346\n",
      "later - Similitud: 0.7635584429852055\n",
      "\n",
      "Palabras similares para el verbo más común del archivo capitulo1libro4.txt:\n",
      "forget - Similitud: 0.858184539994152\n",
      "know - Similitud: 0.8353906035443347\n",
      "tell - Similitud: 0.8222515879358412\n",
      "imagine - Similitud: 0.8196662116552144\n",
      "maybe - Similitud: 0.8183262653679031\n",
      "\n",
      "Palabras similares para el verbo más común del archivo capitulo1libro5.txt:\n",
      "why - Similitud: 0.9440941576101854\n",
      "think - Similitud: 0.9223909012616583\n",
      "n't - Similitud: 0.9213548852280078\n",
      "sure - Similitud: 0.914892210697098\n",
      "you - Similitud: 0.9103051165067565\n",
      "\n",
      "Palabras similares para el verbo más común del archivo capitulo1libro6.txt:\n",
      "told - Similitud: 0.8487459435498669\n",
      "says - Similitud: 0.8161521914923707\n",
      "saying - Similitud: 0.7887590350807986\n",
      "added - Similitud: 0.7826225953331531\n",
      "asked - Similitud: 0.7763409947242162\n",
      "\n",
      "Palabras similares para el verbo más común del archivo capitulo1libro7.txt:\n",
      "going - Similitud: 0.9353025848517753\n",
      "come - Similitud: 0.9278614866424996\n",
      "get - Similitud: 0.8843336527486487\n",
      "'ll - Similitud: 0.8793678312162715\n",
      "do - Similitud: 0.8699576479536059\n",
      "\n",
      "Palabras similares para el verbo más común del archivo capitulo1libro8.txt:\n",
      "look - Similitud: 0.8475161203437067\n",
      "you - Similitud: 0.8381482257243084\n",
      "come - Similitud: 0.8371745006614394\n",
      "how - Similitud: 0.8349792449691887\n",
      "why - Similitud: 0.8311287402834054\n",
      "\n",
      "Palabras similares para el verbo más común del archivo capitulo1libro9.txt:\n",
      "seems - Similitud: 0.8775942229321713\n",
      "seem - Similitud: 0.8744060237073097\n",
      "hardly - Similitud: 0.8368962699703151\n",
      "certainly - Similitud: 0.8214330617783562\n",
      "looked - Similitud: 0.8203958073051721\n",
      "\n",
      "Palabras similares para el verbo más común del archivo capitulo1libro10.txt:\n",
      "told - Similitud: 0.8487459435498669\n",
      "says - Similitud: 0.8161521914923707\n",
      "saying - Similitud: 0.7887590350807986\n",
      "added - Similitud: 0.7826225953331531\n",
      "asked - Similitud: 0.7763409947242162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_diccionario = {}\n",
    "\n",
    "#Cargar los vectores de palabras GloVe\n",
    "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        palabra = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_diccionario[palabra] = vector\n",
    "f.close()\n",
    "\n",
    "# Función para encontrar palabras similares utilizando embeddings de palabras\n",
    "def encontrar_similares(palabra, embeddings_diccionario, n=5):\n",
    "    palabra = palabra.lower()\n",
    "    # Verificar si la palabra está en el diccionario de embeddings\n",
    "    if palabra in embeddings_diccionario:\n",
    "        vector = embeddings_diccionario[palabra]\n",
    "        similares = []\n",
    "        for palabra, vector_palabra in embeddings_diccionario.items():\n",
    "            # Calcular la similitud del coseno entre los vectores\n",
    "            similitud = sp.spatial.distance.cosine(vector, vector_palabra)\n",
    "            similares.append((palabra, similitud))\n",
    "        # Ordenar por similitud y obtener las palabras más similares\n",
    "        similares.sort(key=lambda x: x[1])\n",
    "        return similares[1:n+1]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "# Encontrar palabras similares para cada verbo más común\n",
    "palabras_similares = []\n",
    "print(verbos_mas_comunes)\n",
    "\n",
    "for verbo in verbos_mas_comunes:\n",
    "    palabras_similares.append(encontrar_similares(verbo, embeddings_diccionario))\n",
    "\n",
    "# Imprimir las palabras similares para cada verbo más común\n",
    "print(f'Palabras similares para cada verbo más común:\\n')\n",
    "for i, palabras in enumerate(palabras_similares):\n",
    "    print(f'Palabras similares para el verbo más común del archivo {capitulos[i]}:')\n",
    "    for palabra, similitud in palabras:\n",
    "        print(f'{palabra} - Similitud: {1-similitud}')\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Similitud de documentos con “embedding”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\davic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud entre las frases clave de Dracula y capitulo1libro1.txt:  0.9180206060409546\n",
      "('I had for dinner, or\\r\\nrather supper, a chicken done up some way with red pepper, which was\\r\\nvery good but thirsty.', 10)\n",
      "('They were admirable things for the observer—excellent for\\r\\ndrawing the veil from men’s motives and actions.', 10)\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro2.txt:  0.9333732724189758\n",
      "('I had for dinner, or\\r\\nrather supper, a chicken done up some way with red pepper, which was\\r\\nvery good but thirsty.', 10)\n",
      "('He was induced\\r\\nto look with extreme disapprobation, and at last with distaste, on my\\r\\nfather’s imprudence and follies.', 10)\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro3.txt:  0.9260679483413696\n",
      "('I had for dinner, or\\r\\nrather supper, a chicken done up some way with red pepper, which was\\r\\nvery good but thirsty.', 10)\n",
      "('Beaufort had taken effectual measures to conceal himself, and it was ten\\r\\nmonths before my father discovered his abode.', 10)\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro4.txt:  0.9137712717056274\n",
      "('I had for dinner, or\\r\\nrather supper, a chicken done up some way with red pepper, which was\\r\\nvery good but thirsty.', 10)\n",
      "('The immediate pressure of necessity has brightened\\r\\ntheir intellects, enlarged their powers, and hardened their hearts.', 10)\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro5.txt:  0.9417982697486877\n",
      "('I had for dinner, or\\r\\nrather supper, a chicken done up some way with red pepper, which was\\r\\nvery good but thirsty.', 10)\n",
      "('The place was full, and they wandered about looking for a table,\\r\\ncatching odds and ends of conversation as they did so.\\r\\n\\r\\n', 10)\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro6.txt:  0.9009349942207336\n",
      "('I had for dinner, or\\r\\nrather supper, a chicken done up some way with red pepper, which was\\r\\nvery good but thirsty.', 10)\n",
      "('Having no near relations or friends, I was trying to make\\r\\nup my mind what to do, when I ran across John Cavendish.', 10)\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro7.txt:  0.9041647911071777\n",
      "('I had for dinner, or\\r\\nrather supper, a chicken done up some way with red pepper, which was\\r\\nvery good but thirsty.', 10)\n",
      "('For my part, I abominate all\\r\\nhonorable respectable toils, trials, and tribulations of every kind\\r\\nwhatsoever.', 10)\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro8.txt:  0.9329440593719482\n",
      "('I had for dinner, or\\r\\nrather supper, a chicken done up some way with red pepper, which was\\r\\nvery good but thirsty.', 10)\n",
      "('There was nothing so _very_ remarkable in that; nor did Alice think it\\r\\nso _very_ much out of the way to hear the Rabbit say to itself, “Oh\\r\\ndear!', 10)\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro9.txt:  1.0\n",
      "('I had for dinner, or\\r\\nrather supper, a chicken done up some way with red pepper, which was\\r\\nvery good but thirsty.', 10)\n",
      "('I had for dinner, or\\r\\nrather supper, a chicken done up some way with red pepper, which was\\r\\nvery good but thirsty.', 10)\n",
      "\n",
      "Similitud entre las frases clave de Dracula y capitulo1libro10.txt:  0.9404037594795227\n",
      "('I had for dinner, or\\r\\nrather supper, a chicken done up some way with red pepper, which was\\r\\nvery good but thirsty.', 10)\n",
      "('He had run up against\\r\\nthe ghost on the little staircase, by the footlights, which leads to\\r\\n\"the cellars.\"  ', 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo y el tokenizer de BERT\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "text1 = str(frase_elegida[0])\n",
    "\n",
    "# Iterar sobre cada frase clave y calcular la similitud con el primer capítulo de Dracula\n",
    "for i in range(len(guardar_frases_clave)): \n",
    "    # Codificar las frases clave\n",
    "    tokens1 = tokenizer.encode(text1, return_tensors='pt', truncation=True, padding=True)\n",
    "    tokens2 = tokenizer.encode(str(guardar_frases_clave[i][0]), return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # Obtener embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(input_ids=tokens1)\n",
    "        outputs2 = model(input_ids=tokens2)\n",
    "    \n",
    "    # Obtener el embedding de la primera posición\n",
    "    embedding1 = outputs1.last_hidden_state[:, 0, :].numpy()\n",
    "    embedding2 = outputs2.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "    # Calcular la similitud del coseno entre los embeddings\n",
    "    similarity = np.dot(embedding1.flatten(), embedding2.flatten()) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "    print(f\"Similitud entre las frases clave de Dracula y {capitulos[i]}:  {similarity}\")\n",
    "    print(text1)\n",
    "    print(str(guardar_frases_clave[i][0]))\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
