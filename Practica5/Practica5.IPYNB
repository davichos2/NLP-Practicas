{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Práctica 5. Análisis de sentimientos, polaridad u opinión.\n",
    "\n",
    "Montes de oca Campos David              \n",
    "\n",
    "5BV1\n",
    "\n",
    "Ingeniería en inteligencia artificial\n",
    "\n",
    "Ultima modificación: 26/06/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Analisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\davic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords, opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import pysentiment2 as ps\n",
    "import swifter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset (número de filas, número de columnas): (568454, 10)\n",
      "\n",
      "Descripción de cada dimensión (columna):\n",
      "\n",
      "Columna: Id\n",
      "Propósito: Describir el Id del producto\n",
      "Tipo de dato: int64\n",
      "Formato: Número (entero o decimal)\n",
      "Ejemplo de valores: [1 2 3 4 5]\n",
      "Valor mínimo: 1, Valor máximo: 568454\n",
      "\n",
      "Columna: ProductId\n",
      "Propósito: Describir el ProductId del producto\n",
      "Tipo de dato: object\n",
      "Formato: Cadena de caracteres (texto)\n",
      "Ejemplo de valores: ['B001E4KFG0' 'B00813GRG4' 'B000LQOCH0' 'B000UA0QIQ' 'B006K2ZZ7K']\n",
      "Número de valores únicos: 74258\n",
      "\n",
      "Columna: UserId\n",
      "Propósito: Describir el UserId del producto\n",
      "Tipo de dato: object\n",
      "Formato: Cadena de caracteres (texto)\n",
      "Ejemplo de valores: ['A3SGXH7AUHU8GW' 'A1D87F6ZCVE5NK' 'ABXLMWJIXXAIN' 'A395BORC6FGVXV'\n",
      " 'A1UQRSCLF8GW1T']\n",
      "Número de valores únicos: 256059\n",
      "\n",
      "Columna: ProfileName\n",
      "Propósito: Describir el ProfileName del producto\n",
      "Tipo de dato: object\n",
      "Formato: Cadena de caracteres (texto)\n",
      "Ejemplo de valores: ['delmartian' 'dll pa' 'Natalia Corres \"Natalia Corres\"' 'Karl'\n",
      " 'Michael D. Bigham \"M. Wassir\"']\n",
      "Número de valores únicos: 218415\n",
      "\n",
      "Columna: HelpfulnessNumerator\n",
      "Propósito: Describir el HelpfulnessNumerator del producto\n",
      "Tipo de dato: int64\n",
      "Formato: Número (entero o decimal)\n",
      "Ejemplo de valores: [1 0 3 4 2]\n",
      "Valor mínimo: 0, Valor máximo: 866\n",
      "\n",
      "Columna: HelpfulnessDenominator\n",
      "Propósito: Describir el HelpfulnessDenominator del producto\n",
      "Tipo de dato: int64\n",
      "Formato: Número (entero o decimal)\n",
      "Ejemplo de valores: [1 0 3 4 2]\n",
      "Valor mínimo: 0, Valor máximo: 923\n",
      "\n",
      "Columna: Score\n",
      "Propósito: Describir el Score del producto\n",
      "Tipo de dato: int64\n",
      "Formato: Número (entero o decimal)\n",
      "Ejemplo de valores: [5 1 4 2 3]\n",
      "Valor mínimo: 1, Valor máximo: 5\n",
      "\n",
      "Columna: Time\n",
      "Propósito: Describir el Time del producto\n",
      "Tipo de dato: int64\n",
      "Formato: Número (entero o decimal)\n",
      "Ejemplo de valores: [1303862400 1346976000 1219017600 1307923200 1350777600]\n",
      "Valor mínimo: 939340800, Valor máximo: 1351209600\n",
      "\n",
      "Columna: Summary\n",
      "Propósito: Describir el Summary del producto\n",
      "Tipo de dato: object\n",
      "Formato: Cadena de caracteres (texto)\n",
      "Ejemplo de valores: ['Good Quality Dog Food' 'Not as Advertised' '\"Delight\" says it all'\n",
      " 'Cough Medicine' 'Great taffy']\n",
      "Número de valores únicos: 295742\n",
      "\n",
      "Columna: Text\n",
      "Propósito: Describir el Text del producto\n",
      "Tipo de dato: object\n",
      "Formato: Cadena de caracteres (texto)\n",
      "Ejemplo de valores: ['I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'\n",
      " 'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".'\n",
      " 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.'\n",
      " 'If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.'\n",
      " 'Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.']\n",
      "Número de valores únicos: 393579\n",
      "\n",
      "Primeras 5 filas del dataset:\n",
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cargar el dataset\n",
    "dataset = pd.read_csv('Reviews.csv')\n",
    "\n",
    "#Indicar el número de filas y columnas del dataset\n",
    "print(\"Dimensiones del dataset (número de filas, número de columnas):\", dataset.shape)\n",
    "\n",
    "\n",
    "# Función para describir las columnas\n",
    "def describir_columnas(dataset):\n",
    "    columnas = dataset.columns\n",
    "    for col in columnas:\n",
    "        print(f\"\\nColumna: {col}\")\n",
    "        print(f\"Propósito: Describir el {col} del producto\")\n",
    "        tipo_dato = dataset[col].dtype\n",
    "        print(f\"Tipo de dato: {tipo_dato}\")\n",
    "        if tipo_dato == 'object':\n",
    "            print(\"Formato: Cadena de caracteres (texto)\")\n",
    "            print(f\"Ejemplo de valores: {dataset[col].unique()[:5]}\")\n",
    "            print(f\"Número de valores únicos: {dataset[col].nunique()}\")\n",
    "        elif tipo_dato in ['int64', 'float64']:\n",
    "            print(\"Formato: Número (entero o decimal)\")\n",
    "            print(f\"Ejemplo de valores: {dataset[col].unique()[:5]}\")\n",
    "            print(f\"Valor mínimo: {dataset[col].min()}, Valor máximo: {dataset[col].max()}\")\n",
    "        else:\n",
    "            print(\"Formato: Otro (revisar valores específicos)\")\n",
    "            print(f\"Ejemplo de valores: {dataset[col].unique()[:5]}\")\n",
    "            print(f\"Número de valores únicos: {dataset[col].nunique()}\")\n",
    "\n",
    "# Describir cada columna del dataset\n",
    "print(\"\\nDescripción de cada dimensión (columna):\")\n",
    "describir_columnas(dataset)\n",
    "\n",
    "# Imprimir primeras 5 filas del dataset\n",
    "print(\"\\nPrimeras 5 filas del dataset:\")\n",
    "print(dataset.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Preprocesamiento (Eliminación de columnas, Justificar dimensiones, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dataset después de eliminar columnas no deseadas:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   Id         568454 non-null  int64 \n",
      " 1   ProductId  568454 non-null  object\n",
      " 2   Score      568454 non-null  int64 \n",
      " 3   Summary    568427 non-null  object\n",
      " 4   Text       568454 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 21.7+ MB\n",
      "None\n",
      "\n",
      "Descripción del dataset después de eliminar columnas no deseadas:\n",
      "                  Id          Score\n",
      "count  568454.000000  568454.000000\n",
      "mean   284227.500000       4.183199\n",
      "std    164098.679298       1.310436\n",
      "min         1.000000       1.000000\n",
      "25%    142114.250000       4.000000\n",
      "50%    284227.500000       5.000000\n",
      "75%    426340.750000       5.000000\n",
      "max    568454.000000       5.000000\n",
      "\n",
      "Primeras 5 filas del dataset después de convertir 'Score' a categórico:\n",
      "   Id   ProductId     Score                Summary  \\\n",
      "0   1  B001E4KFG0  Positivo  Good Quality Dog Food   \n",
      "1   2  B00813GRG4  Negativo      Not as Advertised   \n",
      "2   3  B000LQOCH0  Positivo  \"Delight\" says it all   \n",
      "3   4  B000UA0QIQ  Negativo         Cough Medicine   \n",
      "4   5  B006K2ZZ7K  Positivo            Great taffy   \n",
      "\n",
      "                                                Text  \n",
      "0  I have bought several of the Vitality canned d...  \n",
      "1  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  This is a confection that has been around a fe...  \n",
      "3  If you are looking for the secret ingredient i...  \n",
      "4  Great taffy at a great price.  There was a wid...  \n",
      "\n",
      "Balance de clases:\n",
      "Score\n",
      "Positivo    443777\n",
      "Negativo     82037\n",
      "Neutral      42640\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución de clases después de balancear:\n",
      "Score\n",
      "Negativo    42640\n",
      "Neutral     42640\n",
      "Positivo    42640\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Primeras 5 filas del dataset balanceado:\n",
      "            Id   ProductId     Score                   Summary  \\\n",
      "525327  525328  B001AHOZYE  Negativo             Dim Chocolate   \n",
      "75760    75761  B004MO6NI8  Negativo              not the best   \n",
      "468100  468101  B003KFCY06  Negativo                      Yuck   \n",
      "71864    71865  B001E52VLG  Negativo  aspertame is not healthy   \n",
      "211592  211593  B0012KBTW0  Negativo                   Crushed   \n",
      "\n",
      "                                                     Text  \n",
      "525327  I have an absolute passion for deep, dark hot ...  \n",
      "75760   This drink is so \"super energy\" it's almost fr...  \n",
      "468100  I'm sticking with what used to be carnation, n...  \n",
      "71864   Aspertame causes alot of problems including pr...  \n",
      "211592  I ordered these because my local pet store sto...  \n"
     ]
    }
   ],
   "source": [
    "# Eliminar columnas no deseadas\n",
    "columnas_eliminar = ['UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time']\n",
    "dataset = dataset.drop(columnas_eliminar, axis=1)\n",
    "\n",
    "# Mostrar información del dataset después de eliminar columnas\n",
    "print(\"\\n\\nDataset después de eliminar columnas no deseadas:\")\n",
    "print(dataset.info())\n",
    "\n",
    "# Mostrar descripción del dataset después de eliminar columnas\n",
    "print(\"\\nDescripción del dataset después de eliminar columnas no deseadas:\")\n",
    "print(dataset.describe())\n",
    "\n",
    "# Convertir la columna 'Score' a variables categóricas\n",
    "def convertir_score(score):\n",
    "    if score == 1 or score == 2:\n",
    "        return 'Negativo'\n",
    "    elif score == 3:\n",
    "        return 'Neutral'\n",
    "    elif score == 4 or score == 5:\n",
    "        return 'Positivo'\n",
    "    \n",
    "dataset['Score'] = dataset['Score'].apply(convertir_score)\n",
    "\n",
    "# Imprimir primeras 5 filas del dataset después de convertir 'Score' a categórico\n",
    "print(\"\\nPrimeras 5 filas del dataset después de convertir 'Score' a categórico:\")\n",
    "print(dataset.head())\n",
    "\n",
    "#Validar el balance de clases\n",
    "print(\"\\nBalance de clases:\")\n",
    "print(dataset['Score'].value_counts())\n",
    "\n",
    "\n",
    "# Balancear las clases \n",
    "\n",
    "# Separar clases\n",
    "negativo = dataset[dataset['Score'] == 'Negativo']\n",
    "neutral = dataset[dataset['Score'] == 'Neutral']\n",
    "positivo = dataset[dataset['Score'] == 'Positivo']\n",
    "\n",
    "# Encontrar la clase minoritaria\n",
    "min_count = min(len(negativo), len(neutral), len(positivo))\n",
    "\n",
    "# Submuestrear clases a la cantidad de la clase minoritaria\n",
    "negativo_resampled = resample(negativo, replace=False, n_samples=min_count, random_state=42)\n",
    "neutral_resampled = resample(neutral, replace=False, n_samples=min_count, random_state=42)\n",
    "positivo_resampled = resample(positivo, replace=False, n_samples=min_count, random_state=42)\n",
    "\n",
    "# Combinar clases balanceadas\n",
    "dataset_balanced = pd.concat([negativo_resampled, neutral_resampled, positivo_resampled])\n",
    "\n",
    "# Verificar nueva distribución de clases\n",
    "print(\"\\nDistribución de clases después de balancear:\")\n",
    "print(dataset_balanced['Score'].value_counts())\n",
    "\n",
    "# Mostrar primeras 5 filas del dataset balanceado\n",
    "print(\"\\nPrimeras 5 filas del dataset balanceado:\")\n",
    "print(dataset_balanced.head())\n",
    "\n",
    "# Guardar dataset balanceado para análisis posterior\n",
    "dataset_balanced.to_csv('Reviews_balanced.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 y  5. Limpieza de datos y normalización de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 5 filas del dataset después de la limpieza de texto:\n",
      "   Id   ProductId     Score                Summary  \\\n",
      "0   1  B001E4KFG0  Positivo  Good Quality Dog Food   \n",
      "1   2  B00813GRG4  Negativo      Not as Advertised   \n",
      "2   3  B000LQOCH0  Positivo  \"Delight\" says it all   \n",
      "3   4  B000UA0QIQ  Negativo         Cough Medicine   \n",
      "4   5  B006K2ZZ7K  Positivo            Great taffy   \n",
      "\n",
      "                                                Text  \n",
      "0  bought several vitality canned dog food produc...  \n",
      "1  product arrived labeled jumbo salted peanutsth...  \n",
      "2  confection around centuries light pillowy citr...  \n",
      "3  looking secret ingredient robitussin believe f...  \n",
      "4  great taffy great price wide assortment yummy ...  \n",
      "\n",
      "Matriz One-Hot Encoding de las primeras 5 filas:\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Función de limpieza de texto\n",
    "def limpiar_texto(texto):\n",
    "    # Convertir a minúsculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # Eliminar números telefónicos, precios y direcciones (simplificado)\n",
    "    texto = re.sub(r'\\b\\d{10,11}\\b', '', texto)  # Números telefónicos\n",
    "    texto = re.sub(r'\\$\\d+(\\.\\d{2})?', '', texto)  # Precios en formato $xxx.xx\n",
    "    texto = re.sub(r'\\d{1,2} [a-zA-Z]{3,9} [a-zA-Z]{3,9}', '', texto)  # Direcciones\n",
    "    \n",
    "    # Eliminar signos de puntuación\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Eliminar términos alfanuméricos y números\n",
    "    texto = re.sub(r'\\b\\w*\\d\\w*\\b', '', texto)\n",
    "    \n",
    "    # Tokenizar el texto\n",
    "    palabras = word_tokenize(texto)\n",
    "    \n",
    "    # Eliminar stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    palabras_limpias = [word for word in palabras if word not in stop_words]\n",
    "    \n",
    "    # Unir las palabras limpias en una cadena\n",
    "    texto_limpio = ' '.join(palabras_limpias)\n",
    "    \n",
    "    return texto_limpio\n",
    "\n",
    "# Aplicar la función de limpieza a la columna 'Text'\n",
    "dataset['Text'] = dataset['Text'].apply(limpiar_texto)\n",
    "\n",
    "# Imprimir primeras 5 filas del dataset después de la limpieza de texto\n",
    "print(\"\\nPrimeras 5 filas del dataset después de la limpieza de texto:\")\n",
    "print(dataset.head())\n",
    "\n",
    "#Vectorizar las reseñas, usando el método One-Hot Encoding\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Ajustar y transformar la columna 'Score' a One-Hot Encoding a los primeros 1000 registros\n",
    "onehot_encoded = onehot_encoder.fit_transform(dataset['Score'][:1000].values.reshape(-1, 1))\n",
    "\n",
    "# Imprimir las primeras 5 filas de la matriz One-Hot Encoding\n",
    "print(\"\\nMatriz One-Hot Encoding de las primeras 5 filas:\")\n",
    "print(onehot_encoded[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6. Análisis de sentimientos con Harvard y Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 50 filas del dataset después de análisis de sentimientos Harvard IV-4:\n",
      "                                                 Text  Sentimiento_Harvard\n",
      "0   bought several vitality canned dog food produc...             1.000000\n",
      "1   product arrived labeled jumbo salted peanutsth...             0.333333\n",
      "2   confection around centuries light pillowy citr...             0.333333\n",
      "3   looking secret ingredient robitussin believe f...            -0.999999\n",
      "4   great taffy great price wide assortment yummy ...             1.000000\n",
      "5   got wild hair taffy ordered five pound bag taf...            -0.333333\n",
      "6   saltwater taffy great flavors soft chewy candy...             1.000000\n",
      "7   taffy good soft chewy flavors amazing would de...             0.999999\n",
      "8   right im mostly sprouting cats eat grass love ...             0.999999\n",
      "9   healthy dog food good digestion also good smal...             0.000000\n",
      "10  dont know cactus tequila unique combination in...             0.200000\n",
      "11  one boys needed lose weight didnt put food flo...            -1.000000\n",
      "12  cats happily eating felidae platinum two years...             0.000000\n",
      "13  good flavor came securely packed fresh delicio...             0.999999\n",
      "14  strawberry twizzlers guilty pleasure yummy six...             0.000000\n",
      "15  daughter loves twizzlers shipment six pounds r...             0.000000\n",
      "16  love eating good watching tv looking movies sw...             1.000000\n",
      "17  satisfied twizzler purchase shared others enjo...             0.000000\n",
      "18  twizzlers strawberry childhood favorite candy ...             1.000000\n",
      "19  candy delivered fast purchased reasonable pric...             0.333333\n",
      "20  husband twizzlers addict weve bought many time...            -0.333333\n",
      "21  bought husband currently overseas loves appare...             0.000000\n",
      "22  remember buying candy kid quality hasnt droppe...             1.000000\n",
      "23  love candy weight watchers cut back still craving            -0.999999\n",
      "24  lived us miss twizzlers go back visit someone ...             1.000000\n",
      "25  product received advertisedbr br strawberry ba...             0.999999\n",
      "26        candy red flavor plan chewy would never buy             0.999999\n",
      "27  glad amazon carried batteries hard time findin...             0.333333\n",
      "28  got mum diabetic needs watch sugar intake fath...            -0.600000\n",
      "29  dont know cactus tequila unique combination in...             0.200000\n",
      "30  never huge coffee fan however mother purchased...             1.000000\n",
      "31  offer great price great taste thanks amazon se...             1.000000\n",
      "32  mccanns instant oatmeal great must oatmeal scr...             0.692308\n",
      "33  good instant oatmeal best oatmeal brand uses c...             0.333333\n",
      "34  instant oatmeal become soggy minute water hits...            -0.999999\n",
      "35  mccanns instant irish oatmeal variety pack reg...             0.666667\n",
      "36  us celiac disease product lifesaver could bett...             1.000000\n",
      "37  else need know oatmeal instant make half cup l...             0.500000\n",
      "38  visiting friend nate morning coffee came stora...             1.000000\n",
      "39  ordered wife reccomended daughter almost every...             0.000000\n",
      "40  variety packs taste greatbr br every morning c...            -0.333333\n",
      "41  mccanns makes oatmeal every oatmeal connoisseu...            -0.142857\n",
      "42  mccanns oatmeal every morning ordering amazon ...             1.000000\n",
      "43  mccanns oatmeal good quality choice favorite a...            -0.999999\n",
      "44  really like mccanns steel cut oats find dont c...             0.333333\n",
      "45  seems little wholesome supermarket brands some...             0.999999\n",
      "46  good oatmeal like apple cinnamon best though w...             1.000000\n",
      "47  flavors good however see differce oaker oats b...             0.000000\n",
      "48  really like maple brown sugar flavor regular f...            -0.200000\n",
      "49  stuff buy big box stores nothing healthy carbs...             0.333333\n"
     ]
    }
   ],
   "source": [
    "# Crear objeto de análisis de sentimientos\n",
    "hiv4 = ps.HIV4()\n",
    "\n",
    "# Función para obtener el sentimiento de una reseña\n",
    "def obtener_sentimiento_harvard(texto):\n",
    "    tokens = word_tokenize(texto)\n",
    "    sentimiento = hiv4.get_score(tokens)\n",
    "    return sentimiento['Polarity']\n",
    "\n",
    "# Aplicar la función de análisis de sentimientos a la columna 'Text'\n",
    "dataset['Sentimiento_Harvard'] = dataset['Text'].apply(obtener_sentimiento_harvard)\n",
    "\n",
    "# Imprimir primeras 50 filas del dataset después de análisis de sentimientos\n",
    "print(\"\\nPrimeras 50 filas del dataset después de análisis de sentimientos Harvard IV-4:\")\n",
    "print(dataset[['Text', 'Sentimiento_Harvard']].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 568454/568454 [01:05<00:00, 8726.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 50 filas del dataset después de análisis de sentimientos Opinion Lexicon:\n",
      "                                                 Text  Sentimiento_Opinion\n",
      "0   bought several vitality canned dog food produc...                    2\n",
      "1   product arrived labeled jumbo salted peanutsth...                   -1\n",
      "2   confection around centuries light pillowy citr...                    2\n",
      "3   looking secret ingredient robitussin believe f...                    1\n",
      "4   great taffy great price wide assortment yummy ...                    3\n",
      "5   got wild hair taffy ordered five pound bag taf...                    1\n",
      "6   saltwater taffy great flavors soft chewy candy...                    3\n",
      "7   taffy good soft chewy flavors amazing would de...                    5\n",
      "8   right im mostly sprouting cats eat grass love ...                    2\n",
      "9   healthy dog food good digestion also good smal...                    3\n",
      "10  dont know cactus tequila unique combination in...                    8\n",
      "11  one boys needed lose weight didnt put food flo...                   -4\n",
      "12  cats happily eating felidae platinum two years...                    0\n",
      "13  good flavor came securely packed fresh delicio...                    5\n",
      "14  strawberry twizzlers guilty pleasure yummy six...                    0\n",
      "15  daughter loves twizzlers shipment six pounds r...                    1\n",
      "16  love eating good watching tv looking movies sw...                    5\n",
      "17  satisfied twizzler purchase shared others enjo...                    2\n",
      "18  twizzlers strawberry childhood favorite candy ...                    1\n",
      "19  candy delivered fast purchased reasonable pric...                    2\n",
      "20  husband twizzlers addict weve bought many time...                    2\n",
      "21  bought husband currently overseas loves appare...                    5\n",
      "22  remember buying candy kid quality hasnt droppe...                    0\n",
      "23  love candy weight watchers cut back still craving                    1\n",
      "24  lived us miss twizzlers go back visit someone ...                    1\n",
      "25  product received advertisedbr br strawberry ba...                    0\n",
      "26        candy red flavor plan chewy would never buy                    0\n",
      "27  glad amazon carried batteries hard time findin...                    1\n",
      "28  got mum diabetic needs watch sugar intake fath...                    3\n",
      "29  dont know cactus tequila unique combination in...                    8\n",
      "30  never huge coffee fan however mother purchased...                    8\n",
      "31  offer great price great taste thanks amazon se...                    2\n",
      "32  mccanns instant oatmeal great must oatmeal scr...                   10\n",
      "33  good instant oatmeal best oatmeal brand uses c...                    7\n",
      "34  instant oatmeal become soggy minute water hits...                    5\n",
      "35  mccanns instant irish oatmeal variety pack reg...                    6\n",
      "36  us celiac disease product lifesaver could bett...                    3\n",
      "37  else need know oatmeal instant make half cup l...                    1\n",
      "38  visiting friend nate morning coffee came stora...                    2\n",
      "39  ordered wife reccomended daughter almost every...                    3\n",
      "40  variety packs taste greatbr br every morning c...                    2\n",
      "41  mccanns makes oatmeal every oatmeal connoisseu...                    9\n",
      "42  mccanns oatmeal every morning ordering amazon ...                    3\n",
      "43  mccanns oatmeal good quality choice favorite a...                    5\n",
      "44  really like mccanns steel cut oats find dont c...                    3\n",
      "45  seems little wholesome supermarket brands some...                    0\n",
      "46  good oatmeal like apple cinnamon best though w...                    5\n",
      "47  flavors good however see differce oaker oats b...                    0\n",
      "48  really like maple brown sugar flavor regular f...                    5\n",
      "49  stuff buy big box stores nothing healthy carbs...                    1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Función para obtener el sentimiento usando Opinion Lexicon\n",
    "palabras_positivas = set(opinion_lexicon.positive())\n",
    "palabras_negativas = set(opinion_lexicon.negative())\n",
    "\n",
    "def obtener_sentimiento_opinion_lexicon(texto):\n",
    "    tokens = word_tokenize(texto)\n",
    "    sentimiento = 0\n",
    "    for token in tokens:\n",
    "        if token in palabras_positivas:\n",
    "            sentimiento += 1\n",
    "        elif token in palabras_negativas:\n",
    "            sentimiento -= 1\n",
    "    return sentimiento\n",
    "\n",
    "# Aplicar la función de análisis de sentimientos a la columna 'Text' usando swifter para acelerar el proceso\n",
    "dataset['Sentimiento_Opinion'] = dataset['Text'].swifter.apply(obtener_sentimiento_opinion_lexicon)\n",
    "\n",
    "# Imprimir primeras 50 filas del dataset después de análisis de sentimientos\n",
    "print(\"\\nPrimeras 50 filas del dataset después de análisis de sentimientos Opinion Lexicon:\")\n",
    "print(dataset[['Text', 'Sentimiento_Opinion']].head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Análisis de sentimientos con algoritmos de aprendizaje de máquina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\davic\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reporte de clasificación del modelo de Regresión Logística:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negativo       0.63      0.49      0.55     16181\n",
      "     Neutral       0.45      0.08      0.14      8485\n",
      "    Positivo       0.86      0.96      0.91     89025\n",
      "\n",
      "    accuracy                           0.83    113691\n",
      "   macro avg       0.65      0.51      0.53    113691\n",
      "weighted avg       0.80      0.83      0.80    113691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorizar el texto usando TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(dataset['Text'])\n",
    "y = dataset['Score']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar y entrenar el modelo de Regresión Logística\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir las etiquetas de los datos de prueba\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(\"\\nReporte de clasificación del modelo de Regresión Logística:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reporte de clasificación del modelo de Árbol de Decisión con datos reducidos:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negativo       0.49      0.31      0.38     16181\n",
      "     Neutral       0.30      0.13      0.18      8485\n",
      "    Positivo       0.83      0.93      0.88     89025\n",
      "\n",
      "    accuracy                           0.79    113691\n",
      "   macro avg       0.54      0.46      0.48    113691\n",
      "weighted avg       0.75      0.79      0.76    113691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar y entrenar el modelo de Árbol de Decisión con la muestra de datos\n",
    "tree_model = DecisionTreeClassifier(max_depth=30, min_samples_split=10, min_samples_leaf=5)\n",
    "tree_model.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Predecir las etiquetas de los datos de prueba\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(\"\\nReporte de clasificación del modelo de Árbol de Decisión con datos reducidos:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reporte de clasificación del modelo de Máquinas de Soporte Vectorial:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\davic\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\davic\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negativo       0.63      0.46      0.53     16181\n",
      "     Neutral       0.00      0.00      0.00      8485\n",
      "    Positivo       0.85      0.97      0.91     89025\n",
      "\n",
      "    accuracy                           0.83    113691\n",
      "   macro avg       0.49      0.48      0.48    113691\n",
      "weighted avg       0.75      0.83      0.78    113691\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\davic\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Convertir etiquetas one-hot a 1D si es necesario\n",
    "if len(y.shape) > 1 and y.shape[1] > 1:\n",
    "    y = np.argmax(y.values, axis=1)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=0.2, random_state=42)  \n",
    "\n",
    "# Inicializar el modelo de Máquinas de Soporte Vectorial\n",
    "svm_model = SVC(kernel='linear', C=1, gamma='scale')\n",
    "\n",
    "\n",
    "# Entrenar el modelo con todos los datos de entrenamiento\n",
    "svm_model.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Predecir las etiquetas de los datos de prueba\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(\"\\nReporte de clasificación del modelo de Máquinas de Soporte Vectorial:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Puntajes de cross-validation:\n",
      "Regresión Logística: [0.33336667 0.33336667]\n",
      "Árbol de Decisión: [0.43883333 0.43663333]\n",
      "Máquinas de Soporte Vectorial: [0.3806     0.38026667]\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset balanceado\n",
    "dataset = pd.read_csv('Reviews_balanced.csv')\n",
    "\n",
    "# seleccionar un subconjunto de ejemplos de cada clase\n",
    "subset_size = 20000\n",
    "balanced_dataset = pd.DataFrame()\n",
    "\n",
    "# Iterar sobre cada clase y seleccionar subset_size ejemplos de cada una\n",
    "for score in dataset['Score'].unique():\n",
    "    subset = dataset[dataset['Score'] == score].head(subset_size)\n",
    "    balanced_dataset = pd.concat([balanced_dataset, subset], ignore_index=True)\n",
    "\n",
    "# Cargar X y y del dataset balanceado\n",
    "X = balanced_dataset['Text']\n",
    "y = balanced_dataset['Score']\n",
    "\n",
    "# Aplicar TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "# Reducir dimensionalidad con TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=1)  \n",
    "X_reduced = svd.fit_transform(X_tfidf)\n",
    "\n",
    "# Inicializar modelos\n",
    "logistic_model = LogisticRegression(max_iter=1)\n",
    "tree_model = DecisionTreeClassifier()\n",
    "svm_model = SVC()\n",
    "\n",
    "# Paralelización con cross_val_score usando n_jobs=-1\n",
    "try:\n",
    "    logistic_scores = cross_val_score(logistic_model, X_reduced, y, cv=2, n_jobs=-1)\n",
    "    tree_scores = cross_val_score(tree_model, X_reduced, y, cv=2, n_jobs=-1)\n",
    "    svm_scores = cross_val_score(svm_model, X_reduced, y, cv=2, n_jobs=-1)\n",
    "\n",
    "    # Imprimir los puntajes de cross-validation\n",
    "    print(\"\\nPuntajes de cross-validation:\")\n",
    "    print(\"Regresión Logística:\", logistic_scores)\n",
    "    print(\"Árbol de Decisión:\", tree_scores)\n",
    "    print(\"Máquinas de Soporte Vectorial:\", svm_scores)\n",
    "except ValueError as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\davic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6ms/step - accuracy: 0.8410 - loss: 0.4498 - val_accuracy: 0.8769 - val_loss: 0.3306\n",
      "Epoch 2/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 6ms/step - accuracy: 0.8853 - loss: 0.3126 - val_accuracy: 0.8910 - val_loss: 0.3053\n",
      "Epoch 3/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6ms/step - accuracy: 0.9069 - loss: 0.2547 - val_accuracy: 0.8955 - val_loss: 0.3019\n",
      "Epoch 4/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 5ms/step - accuracy: 0.9229 - loss: 0.2103 - val_accuracy: 0.8975 - val_loss: 0.3061\n",
      "Epoch 5/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8874s\u001b[0m 624ms/step - accuracy: 0.9345 - loss: 0.1774 - val_accuracy: 0.8977 - val_loss: 0.3395\n",
      "Epoch 6/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 12ms/step - accuracy: 0.9435 - loss: 0.1516 - val_accuracy: 0.8993 - val_loss: 0.3871\n",
      "Epoch 7/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 14ms/step - accuracy: 0.9516 - loss: 0.1299 - val_accuracy: 0.8986 - val_loss: 0.3904\n",
      "Epoch 8/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 11ms/step - accuracy: 0.9573 - loss: 0.1150 - val_accuracy: 0.8967 - val_loss: 0.4364\n",
      "Epoch 9/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 16ms/step - accuracy: 0.9628 - loss: 0.1028 - val_accuracy: 0.8970 - val_loss: 0.4800\n",
      "Epoch 10/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 16ms/step - accuracy: 0.9683 - loss: 0.0881 - val_accuracy: 0.8961 - val_loss: 0.5423\n",
      "Epoch 11/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 16ms/step - accuracy: 0.9708 - loss: 0.0810 - val_accuracy: 0.8950 - val_loss: 0.5863\n",
      "Epoch 12/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 15ms/step - accuracy: 0.9741 - loss: 0.0731 - val_accuracy: 0.8949 - val_loss: 0.5575\n",
      "Epoch 13/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 15ms/step - accuracy: 0.9764 - loss: 0.0673 - val_accuracy: 0.8844 - val_loss: 0.6074\n",
      "Epoch 14/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 13ms/step - accuracy: 0.9782 - loss: 0.0623 - val_accuracy: 0.8927 - val_loss: 0.6168\n",
      "Epoch 15/15\n",
      "\u001b[1m14212/14212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 11ms/step - accuracy: 0.9798 - loss: 0.0582 - val_accuracy: 0.8901 - val_loss: 0.7200\n",
      "\u001b[1m3553/3553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.890088045667643\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='')\n",
    "tokenizer.fit_on_texts(dataset['Text'])\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(dataset['Text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100, truncating='post')\n",
    "\n",
    "# Convertir etiquetas de sentimiento a one-hot encoding\n",
    "labels = pd.get_dummies(dataset['Score'])\n",
    "\n",
    "# Asegurarse de que las dimensiones coincidan\n",
    "assert len(padded_sequences) == len(labels), \"Las dimensiones de padded_sequences y labels no coinciden.\"\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Definir el modelo\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 100, input_length=100))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=-1), y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save('sentiment_analysis_model.h5')\n",
    "\n",
    "# Guardar el tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Texto: bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n",
      "Sentimiento: Positive\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Texto: product arrived labeled jumbo salted peanutsthe peanuts actually small sized unsalted sure error vendor intended represent product jumbo\n",
      "Sentimiento: Negative\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Texto: confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story cs lewis lion witch wardrobe treat seduces edmund selling brother sisters witch\n",
      "Sentimiento: Positive\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Texto: looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal\n",
      "Sentimiento: Negative\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Texto: great taffy great price wide assortment yummy taffy delivery quick taffy lover deal\n",
      "Sentimiento: Positive\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Texto: got wild hair taffy ordered five pound bag taffy enjoyable many flavors watermelon root beer melon peppermint grape etc complaint bit much redblack licoriceflavored pieces particular favorites kids husband lasted two weeks would recommend brand taffy delightful treat\n",
      "Sentimiento: Positive\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Texto: saltwater taffy great flavors soft chewy candy individually wrapped well none candies stuck together happen expensive version fralingers would highly recommend candy served beachthemed party everyone loved\n",
      "Sentimiento: Positive\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Texto: taffy good soft chewy flavors amazing would definitely recommend buying satisfying\n",
      "Sentimiento: Positive\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Texto: right im mostly sprouting cats eat grass love rotate around wheatgrass rye\n",
      "Sentimiento: Positive\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Texto: healthy dog food good digestion also good small puppies dog eats required amount every feeding\n",
      "Sentimiento: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('sentiment_analysis_model.h5')\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)\n",
    "\n",
    "# Definir una función para predecir el sentimiento del texto de entrada\n",
    "def predict_sentiment(text):\n",
    "    # Tokenizar y hacer padding del texto de entrada\n",
    "    text_sequence = loaded_tokenizer.texts_to_sequences([text])\n",
    "    text_sequence = pad_sequences(text_sequence, maxlen=100)\n",
    "\n",
    "    # Hacer una predicción usando el modelo cargado\n",
    "    predicted_sentiment = loaded_model.predict(text_sequence)\n",
    "    sentiment_label = np.argmax(predicted_sentiment)\n",
    "    \n",
    "    # Mapear el índice de la etiqueta de sentimiento a su categoría correspondiente\n",
    "    if sentiment_label == 0:\n",
    "        return 'Negative'\n",
    "    elif sentiment_label == 1:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "#Uso de la función predict_sentiment\n",
    "# Pasar 10 reseñas de text del dataset para predecir el sentimiento\n",
    "for i in range(10):\n",
    "    text = dataset['Text'][i]\n",
    "    sentiment = predict_sentiment(text)\n",
    "    print(f\"Texto: {text}\")\n",
    "    print(f\"Sentimiento: {sentiment}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Crear la matriz de embeddings\u001b[39;00m\n\u001b[0;32m     21\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m---> 22\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[43mvocab_size\u001b[49m, embedding_dim))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, i \u001b[38;5;129;01min\u001b[39;00m word_index\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     24\u001b[0m     embedding_vector \u001b[38;5;241m=\u001b[39m embeddings_index\u001b[38;5;241m.\u001b[39mget(word)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset balanceado\n",
    "dataset = pd.read_csv('Reviews_balanced.csv')\n",
    "\n",
    "# Convertir los documentos a secuencias de enteros\n",
    "sequences = tokenizer.texts_to_sequences(dataset['Text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100, truncating='post')\n",
    "\n",
    "# Convertir etiquetas de sentimiento a one-hot encoding\n",
    "labels = pd.get_dummies(dataset['Score']).values\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Cargar los embeddings preentrenados (GloVe)\n",
    "embeddings_index = dict()\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Crear la matriz de embeddings\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Definir el modelo\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=-1), y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save('sentiment_analysis_model.h5')\n",
    "\n",
    "# Guardar el tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "loaded_model = load_model('sentiment_analysis_model.h5')\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)\n",
    "\n",
    "# Definir una función para predecir el sentimiento del texto de entrada\n",
    "def predict_sentiment(text):\n",
    "    # Tokenizar y hacer padding del texto de entrada\n",
    "    text_sequence = loaded_tokenizer.texts_to_sequences([text])\n",
    "    text_sequence = pad_sequences(text_sequence, maxlen=100)\n",
    "\n",
    "    # Hacer una predicción usando el modelo cargado\n",
    "    predicted_sentiment = loaded_model.predict(text_sequence)\n",
    "    sentiment_label = np.argmax(predicted_sentiment)\n",
    "    \n",
    "    # Mapear el índice de la etiqueta de sentimiento a su categoría correspondiente\n",
    "    if sentiment_label == 0:\n",
    "        return 'Negative'\n",
    "    elif sentiment_label == 1:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "# Ejemplo de uso\n",
    "text_input = \"I have an absolute passion for deep, dark hot chocolate. I want chocolate which tastes like *chocolate*, rich and flavorful, with a hint of a bite. You can savor chocolate like that.\"\n",
    "predicted_sentiment = predict_sentiment(text_input)\n",
    "print(predicted_sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
